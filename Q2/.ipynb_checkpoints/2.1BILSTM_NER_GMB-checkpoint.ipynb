{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../NER_Dataset/ner-gmb/train.txt\", sep = \" \", header = None, names = ['token', 'postag', 'word', 'nertag'], skip_blank_lines=False)\n",
    "# df_list = np.split(df, df[df.isnull().all(1)].index) \n",
    "# #add sentence number to each dataframe and then combine. Also for each dataframe remove the first row\n",
    "\n",
    "# def add_sentence_number(df, i):\n",
    "#     '''adds a column to df in which all rows have value i'''\n",
    "#     df['sentence'] = i\n",
    "#     return df\n",
    "\n",
    "# new_dflist = []\n",
    "# for i, df in enumerate(df_list[1:]):\n",
    "#     df.reset_index(drop=True, inplace = True)\n",
    "#     df.dropna(inplace = True)\n",
    "#     df = add_sentence_number(df, i)\n",
    "#     new_dflist.append(df)\n",
    "\n",
    "# len(new_dflist)\n",
    "\n",
    "# def combine_dataframes(df_list):\n",
    "\n",
    "#     return pd.concat(df_list)\n",
    "\n",
    "\n",
    "# df = combine_dataframes(new_dflist)\n",
    "# df.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading text file in python and making list of sentences (list of lists) and list of tags(list of lists)\n",
    "\n",
    "traindatapath = \"../NER_Dataset/ner-gmb/train.txt\"\n",
    "# train_sent = []\n",
    "# train_tags = []\n",
    "all_words = []\n",
    "all_tags = []\n",
    "with open(traindatapath) as f:\n",
    "    lines = f.readlines()\n",
    "    sent_num = 0\n",
    "    for line in lines[1:]: #1: so that the first blank line isn't taken into account\n",
    "        if(line == \"\\n\"):\n",
    "            sent_num+=1\n",
    "            train_sent.append(sentence)\n",
    "            train_tags.append(tags)\n",
    "            sentence = []\n",
    "            tags = []\n",
    "        else:\n",
    "            line_sep = line.split(sep = \" \")\n",
    "            all_words.append(line_sep[0])\n",
    "            all_tags.append(line_sep[3][:-1])\n",
    "            \n",
    "words = list(set(all_words))\n",
    "tags = list(set(all_tags))\n",
    "\n",
    "vocab = {}\n",
    "vocab['<pad>'] = 0 # for padding input sequences\n",
    "for i, word in enumerate(words):\n",
    "    vocab[word] = i+1\n",
    "    \n",
    "nertags = {}\n",
    "nertags['padtag'] = 0\n",
    "for i,nertag in enumerate(tags):\n",
    "    nertags[nertag] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent = []\n",
    "train_tags = []\n",
    "with open(traindatapath) as f:\n",
    "    lines = f.readlines()\n",
    "    sent_num = 0\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    for line in lines[1:]: #1: so that the first blank line isn't taken into account\n",
    "        if(line == \"\\n\"):\n",
    "            sent_num+=1\n",
    "            train_sent.append(sentence)\n",
    "            train_tags.append(tags)\n",
    "            sentence = []\n",
    "            tags = []\n",
    "        else:\n",
    "            line_sep = line.split(sep = \" \")\n",
    "            sentence.append(vocab[line_sep[0]])\n",
    "            tags.append(nertags[line_sep[3][:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4005, 19151, 1187, 3403, 20949, 18982, 11604, 4005, 10587, 17286, 22237, 1010, 9697, 22237, 22227, 17286, 752, 12471, 18281, 6116, 18268, 23439, 16756, 659, 14017, 15937, 13748, 4005, 6014, 22244, 20949, 13748, 12471, 3962, 23439, 2606, 7225, 21870, 20168, 7997, 13403], [14355, 20949, 4005, 699, 12471, 16866, 32, 21932, 6370, 9270, 21870, 7362, 21328, 9878, 14066, 4005, 10281, 23439, 4005, 16677, 8864, 13748, 6200, 13403]]\n",
      "[[16, 9, 16, 16, 16, 4, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 16, 16, 16, 16], [16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 16, 6, 16, 16, 16, 16, 16, 16, 16, 13, 16, 16, 9, 16]]\n",
      "24136 unique words\n",
      "0 unique tags\n"
     ]
    }
   ],
   "source": [
    "print(train_sent[:2])\n",
    "print(train_tags[:2])\n",
    "print(len(words), \"unique words\")\n",
    "print(\"{} unique tags\".format(len(tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the sentences at the end\n",
    "seq_maxlen = max(len(x) for x in train_sent)\n",
    "x_lengths = [len(x) for x in train_sent]\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "for sent, tags in zip(train_sent, train_tags):\n",
    "    length_toappend = seq_maxlen - len(sent)\n",
    "    Xtrain.append(sent+[0]*length_toappend)\n",
    "    Ytrain.append(tags+[0]*length_toappend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4005, 19151, 1187, 3403, 20949, 18982, 11604, 4005, 10587, 17286, 22237, 1010, 9697, 22237, 22227, 17286, 752, 12471, 18281, 6116, 18268, 23439, 16756, 659, 14017, 15937, 13748, 4005, 6014, 22244, 20949, 13748, 12471, 3962, 23439, 2606, 7225, 21870, 20168, 7997, 13403, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[16, 9, 16, 16, 16, 4, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 16, 16, 16, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[0])\n",
    "print(Ytrain[0])\n",
    "print(x_lengths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37206, 104]) torch.Size([37206, 104]) torch.Size([37206])\n"
     ]
    }
   ],
   "source": [
    "Xtrain = torch.Tensor(Xtrain)\n",
    "Ytrain = torch.Tensor(Ytrain)\n",
    "x_lengths = torch.Tensor(x_lengths)\n",
    "print(Xtrain.shape, Ytrain.shape, x_lengths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using dataloader to make data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(Xtrain, Ytrain, x_lengths)\n",
    "dataloader = DataLoader(dataset, batch_size= 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 104])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(next(iter(dataloader))))\n",
    "next(iter(dataloader))[0].shape #shape is (batchsize, seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random initialization of word embeddings\n",
    "# # ie define tensor of size (number of words, embedding dimesnion)\n",
    "# embed_dim = 100\n",
    "# num_words = len(vocab)\n",
    "# random_embed = torch.rand(num_words, embed_dim)\n",
    "# print(random_embed.shape)\n",
    "# print(random_embed[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, total_words, num_class, pretrained = False, pretrained_embed = None):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.wordembed = nn.Embedding(total_words, embedding_size) #weights initiallized from std normal dist, randomly\n",
    "        if(pretrained == True):\n",
    "            self.wordembed.weight = nn.Parameter(pretrained_embed) #pretrained embeds have size (total_words, embedding_size)\n",
    "\n",
    "        self.bilstm = nn.LSTM(embedding_size,hidden_size, bidirectional = True, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, num_class)\n",
    "\n",
    "    def forward(self, x, xlengths):\n",
    "        out = self.wordembed(x) # x is of size(batchsize, seq_len), out is of size (batchsize, seq_len, embedding_size = 100)\n",
    "        \n",
    "        packed_out = pack_padded_sequence(out, xlengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        out, (h,c) = self.bilstm(packed_out) #'out' has dimension(batchsize, seq_len, hidden_size)\n",
    "        \n",
    "        out, out_lengths = pad_packed_sequence(out, batch_first=True)\n",
    "        \n",
    "        out = self.linear(out) #'out' has dimension(batchsize, seq_len, num_class)\n",
    "\n",
    "        out = F.softmax(out, dim=2) # take the softmax across the dimension num_class\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(embedding_size = 100, hidden_size = 100, total_words = len(vocab), num_class = 17, pretrained = False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (wordembed): Embedding(24137, 100)\n",
      "  (bilstm): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=100, out_features=17, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is ready now we have to train using cross entropy loss\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "total_batches = len(train_sent)//128 +1\n",
    "trainloss = []\n",
    "# validloss = []\n",
    "for epoch in num_epochs:\n",
    "    for step, (Xbatch, Ybatch, xbatch_len) in enumerate(train_loader):\n",
    "        #make gradients 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #get output from model and claculate loss\n",
    "        yhat_batch = model(Xbatch, xbatch_len)\n",
    "        loss = lossfunction(Ybatch, yhat_batch)\n",
    "        trainloss.append(loss)\n",
    "        \n",
    "        #backward and step\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5) # clip gradient to 5\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
