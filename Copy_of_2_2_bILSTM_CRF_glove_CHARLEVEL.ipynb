{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of 2.2_bILSTM_CRF_glove_CHARLEVEL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul5fA8hKT9LT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939ede7a-86f9-496f-e73a-d294115e34ee"
      },
      "source": [
        "Expname = \"BiLSTM_crf_char_glove_layernorm\"\n",
        "pre_embeddings= \"glove\" # glove or random\n",
        "rootpath = \"/content/drive/MyDrive/Q2_DL/Experiments/\"\n",
        "\n",
        "!pip install seqeval\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import io\n",
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seqeval\n",
        "from seqeval.metrics import accuracy_score as seq_accuracy_score\n",
        "from seqeval.metrics import classification_report as seq_classification_report\n",
        "from seqeval.metrics import f1_score as seq_f1_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 26.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 23.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 17.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=60975cdea86fc90d8f50eeab2625599541c5b6ee366550f1972d3452462aa5e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpFwO8IBUIHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feaecfdc-1f50-475f-9232-a3ee8130a211"
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "  device = \"cuda:0\" \n",
        "else:  \n",
        "  device = \"cpu\"  \n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsTk933f2NFE"
      },
      "source": [
        "'''Helper Functions'''\n",
        "# reading text file in python and making list of sentences (list of lists) and list of tags(list of lists)\n",
        "def load_data(datapath, buildvocab_tags= True, vocab = None, nertags = None):\n",
        "    if(buildvocab_tags == True):\n",
        "        all_words = []\n",
        "        all_tags = []\n",
        "        with open(datapath) as f:\n",
        "            lines = f.readlines()\n",
        "            sent_num = 0\n",
        "            for line in lines[1:]: #1: so that the first blank line isn't taken into account\n",
        "                if(line == \"\\n\"):\n",
        "                    sent_num+=1\n",
        "                else:\n",
        "                    line_sep = line.split(sep = \" \")\n",
        "                    all_words.append(line_sep[0])\n",
        "                    all_tags.append(line_sep[3][:-1])\n",
        "                    \n",
        "        words = list(set(all_words))\n",
        "        tags = list(set(all_tags))\n",
        "\n",
        "        vocab = {}\n",
        "        vocab['<pad>'] = 0 # for padding input sequences\n",
        "        vocab['<oov>'] = 1\n",
        "        for i, word in enumerate(words):\n",
        "            vocab[word] = i+2\n",
        "            \n",
        "        nertags = {}\n",
        "        nertags['padtag'] = 0\n",
        "        for i,nertag in enumerate(tags):\n",
        "            nertags[nertag] = i+1\n",
        "\n",
        "    train_sent = []\n",
        "    train_tags = []\n",
        "    with open(datapath) as f:\n",
        "        lines = f.readlines()\n",
        "        sent_num = 0\n",
        "        sentence = []\n",
        "        tag = []\n",
        "        for line in lines[1:]: #1: so that the first blank line isn't taken into account\n",
        "            if(line == \"\\n\"):\n",
        "                sent_num+=1\n",
        "                train_sent.append(sentence)\n",
        "                train_tags.append(tag)\n",
        "                sentence = []\n",
        "                tag = []\n",
        "            else:\n",
        "                line_sep = line.split(sep = \" \")\n",
        "                if(line_sep[0] in vocab.keys()):\n",
        "                    sentence.append(vocab[line_sep[0]])\n",
        "                else:\n",
        "                    sentence.append(vocab['<oov>'])\n",
        "                    \n",
        "                tag.append(nertags[line_sep[3][:-1]])\n",
        "\n",
        "    # padding the sentences at the end\n",
        "    seq_maxlen = max(len(x) for x in train_sent)\n",
        "    x_lengths = [len(x) for x in train_sent]\n",
        "    Xtrain = []\n",
        "    Ytrain = []\n",
        "    for sent, tags in zip(train_sent, train_tags):\n",
        "        length_toappend = seq_maxlen - len(sent)\n",
        "        Xtrain.append(sent+[0]*length_toappend)\n",
        "        Ytrain.append(tags+[0]*length_toappend)\n",
        "\n",
        "\n",
        "    Xtrain = torch.Tensor(Xtrain)\n",
        "    Ytrain = torch.Tensor(Ytrain)\n",
        "    x_lengths = torch.Tensor(x_lengths)\n",
        "    # print(Xtrain.shape, Ytrain.shape, x_lengths.shape)\n",
        "    \n",
        "    return Xtrain, Ytrain, x_lengths, vocab, nertags\n",
        "\n",
        "def get_mask(x, xlengths):\n",
        "    bin_mask = []\n",
        "    for i in range(xlengths.shape[0]):\n",
        "        bin_mask.append([1]*int(xlengths[i].item())+[0]*int((x.shape[1] - xlengths[i].item())))\n",
        "    return torch.tensor(bin_mask)\n",
        "\n",
        "def load_char_data(words, charvocab):\n",
        "    train_char_sent = []\n",
        "    train_char_label = []\n",
        "    for word in words:\n",
        "        chars = []\n",
        "        char_labels = []\n",
        "\n",
        "        word_sep = list(word)\n",
        "        for c in word_sep[:-1]:\n",
        "            if (c in charvocab.keys()):\n",
        "                chars.append(charvocab[c])\n",
        "            else:\n",
        "                chars.append(charvocab['<oovchar>'])\n",
        "        for c in word_sep[1:]:\n",
        "            if (c in charvocab.keys()):\n",
        "                char_labels.append(charvocab[c])\n",
        "            else:\n",
        "                char_labels.append(charvocab['<oovchar>'])\n",
        "        \n",
        "        train_char_sent.append(chars)\n",
        "        train_char_label.append(char_labels)\n",
        "\n",
        "    # padding the char_sents at the end\n",
        "    seq_maxlen = max(len(x) for x in train_char_sent)\n",
        "    x_lengths_char = [len(x) for x in train_char_sent]\n",
        "    Xtrain_char = []\n",
        "    Ytrain_char = []\n",
        "    for char_sent, char_label in zip(train_char_sent, train_char_label):\n",
        "        length_toappend = seq_maxlen - len(char_sent)\n",
        "        Xtrain_char.append(char_sent+[0]*length_toappend)\n",
        "        Ytrain_char.append(char_label+[0]*length_toappend) # 0 is padchar\n",
        "\n",
        "\n",
        "    Xtrain_char = torch.Tensor(Xtrain_char)\n",
        "    Ytrain_char = torch.Tensor(Ytrain_char)\n",
        "    x_lengths_char = torch.Tensor(x_lengths_char)\n",
        "    # print(Xtrain.shape, Ytrain.shape, x_lengths.shape)\n",
        "    \n",
        "    return Xtrain_char, Ytrain_char, x_lengths_char\n",
        "\n",
        "def pad_chars(topadlist, maxlen):\n",
        "    topadlist = topadlist + [0]*(maxlen-len(topadlist))\n",
        "\n",
        "    return topadlist\n",
        "\n",
        "def make_id2word_charvocab(vocab, charvocab):\n",
        "    max_charlen = max(len(word) for word in vocab.keys())\n",
        "    word_charlevel_vocab = {}\n",
        "    wordid2wordlen = {}\n",
        "    for word in vocab.keys():\n",
        "        word_charlevel_vocab[vocab[word]] = [charvocab[w] if w in charvocab.keys() else charvocab['<oovchar>'] for w in word]\n",
        "        word_charlevel_vocab[vocab[word]] = pad_chars(word_charlevel_vocab[vocab[word]], max_charlen)\n",
        "\n",
        "        wordid2wordlen[vocab[word]] = len(word)\n",
        "        # word_charlevel_vocab[vocab[word]] = word_charlevel_vocab[vocab[word]].extend([charvocab['<padchar>']]*(max_charlen-len(word_charlevel_vocab[vocab[word]])))\n",
        "    return word_charlevel_vocab, wordid2wordlen\n",
        "\n",
        "\n",
        "def load_char_level(X, wordid2word_charlevel_vocab, wordid2wordlen):\n",
        "    #X is of shape (no.of.sentences, 104)\n",
        "    Xcharlevel = [] # will finally be fo shape (total.sentences, max_sent.len, )\n",
        "    Xcharlevel_lengths = []\n",
        "    for i in range(X.shape[0]):\n",
        "        sentence = []\n",
        "        wordlengths = []\n",
        "        for j in range(X.shape[1]):\n",
        "            sentence.append(torch.tensor([wordid2word_charlevel_vocab[int(X[i, j].item())]]))\n",
        "            wordlengths.append(wordid2wordlen[int(X[i, j].item())])\n",
        "            # sentences = pad_sequence(sentences)\n",
        "        # print(i)\n",
        "        Xcharlevel_lengths.append(wordlengths)\n",
        "        Xcharlevel.append(torch.stack(sentence))\n",
        "    \n",
        "    return torch.squeeze(torch.stack(Xcharlevel)), torch.tensor(Xcharlevel_lengths)\n",
        "\n",
        "def get_charvocab(vocab):\n",
        "    # using vocab make charvocab\n",
        "    words = list(vocab.keys())\n",
        "    characters = [char for word in words for char in word]\n",
        "    characters = list(set(characters))\n",
        "    char_vocab = {}\n",
        "    char_vocab[\"<padchar>\"] = 0\n",
        "    char_vocab[\"<oovchar>\"] = 1\n",
        "    for i, char in enumerate(characters):\n",
        "        char_vocab[char] = i+2\n",
        "\n",
        "    return char_vocab\n",
        "\n",
        "\n",
        "\"\"\"### Training Data\n",
        "### using dataloader to make data batches\"\"\"\n",
        "\n",
        "traindatapath = \"/content/drive/MyDrive/Q2_DL/train.txt\"\n",
        "devdatapath = \"/content/drive/MyDrive/Q2_DL/dev.txt\"\n",
        "\n",
        "Xtrain, Ytrain, x_trainlengths, vocab, nertags = load_data(traindatapath, buildvocab_tags=True)\n",
        "Xdev, Ydev, x_devlengths, _, _ = load_data(devdatapath, buildvocab_tags=False, vocab = vocab, nertags = nertags)\n",
        "\n",
        "bin_mask_train, bin_mask_dev = get_mask(Xtrain, x_trainlengths), get_mask(Xdev, x_devlengths)\n",
        "\n",
        "# Character Level training data making\n",
        "# make vocabulary of characters from train vocabulary\n",
        "char_vocab = get_charvocab(vocab)\n",
        "wordid2word_charlevel_vocab, wordid2wordlen = make_id2word_charvocab(vocab, char_vocab) # of the form {word:[1,2,3,4]}, {wordnum:wordlen}\n",
        "#make char level train data for the char embeddings \n",
        "Xtrain_char, xlength_char = load_char_level(Xtrain, wordid2word_charlevel_vocab, wordid2wordlen)\n",
        "#finally make the dataloader for train\n",
        "traindataset = TensorDataset(Xtrain, Xtrain_char, Ytrain, x_trainlengths, xlength_char, bin_mask_train)\n",
        "Trainloader = DataLoader(traindataset, batch_size= 128, shuffle=True)\n",
        "\n",
        "\n",
        "# Character Level validation data making\n",
        "Xdev_temp, Ydev_temp, x_devlengths_temp, devvocab, devnertags = load_data(devdatapath, buildvocab_tags=True)\n",
        "wordid2word_charlevel_vocab_dev, wordid2wordlen_dev = make_id2word_charvocab(devvocab, char_vocab) # of the form {word:[1,2,3,4]}, {wordnum:wordlen}\n",
        "#make char level train data for the char embeddings \n",
        "Xdev_char, xdevlength_char = load_char_level(Xdev_temp, wordid2word_charlevel_vocab_dev, wordid2wordlen_dev)\n",
        "#finally make the dataloader for train\n",
        "devdataset = TensorDataset(Xdev, Xdev_char, Ydev, x_devlengths, xdevlength_char, bin_mask_dev)\n",
        "Devloader = DataLoader(devdataset, batch_size= 128, shuffle=True)\n",
        "\n",
        "# LOAD EMBEDDINGS\n",
        "embedding_size = 100\n",
        "if(pre_embeddings == \"glove\"):\n",
        "    gloveembeddings_index = {}\n",
        "    with io.open(\"/content/drive/MyDrive/Q2_DL/glove.6B.100d.txt\", encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:],dtype='float32')\n",
        "            gloveembeddings_index[word] = coefs\n",
        "\n",
        "    #using vocab and Xtrain, Xvalid, get pretrained glove word embeddings\n",
        "    glove_embeds = np.zeros((len(vocab), embedding_size))\n",
        "    for word in vocab.keys():\n",
        "        if(word in gloveembeddings_index.keys()):\n",
        "            # for the pad word let theembedding be all zeros\n",
        "            glove_embeds[vocab[word]] = gloveembeddings_index[word]\n",
        "        else:\n",
        "            glove_embeds[vocab[word]] = np.random.randn(embedding_size)\n",
        "    word_embeds = torch.Tensor(glove_embeds)\n",
        "    # print(glove_embeds.shape) # shape (vocablength , embedding dim)\n",
        "\n",
        "if(pre_embeddings == \"random\"):\n",
        "    num_words = len(vocab)\n",
        "    word_embeds = torch.rand(num_words, embedding_size)\n",
        "\n",
        "# hence we get word_embeds which we could use afterwards\n",
        "\n",
        "\n",
        "# character level onehot embeddings and important classes for performance metrics\n",
        "char_onehot = torch.eye(len(char_vocab))\n",
        "imp_classes = [nertags[tag] for tag in nertags.keys()]\n",
        "imp_classes.remove(nertags['padtag'])\n",
        "imp_classes.remove(nertags['O'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9M1uW9_xHfE"
      },
      "source": [
        "\"\"\"### CRF MODULE\"\"\"\n",
        "\n",
        "class CRFmodule(nn.Module):\n",
        "    def __init__(self, numclass):\n",
        "        super(CRFmodule, self).__init__()\n",
        "        # numtags is representing the start position and numtags+1 the end position/id of the tags\n",
        "        random_transition = torch.zeros(numclass+2, numclass+2)\n",
        "        random_transition[numclass, :], random_transition[:, numclass+1] = -10000., -10000.\n",
        "        self.transmat = nn.Parameter(random_transition) # learnable matrix shape (num_classes, numclasses) including start and stop tag\n",
        "        self.startid = numclass\n",
        "        self.endid = numclass+1\n",
        "    \n",
        "    def forward(self, Ylstm, Ymask):\n",
        "        # y of shape (batchsize, seq_len, num_classes) num_classes = 18 when start and stop are not included but pad is included\n",
        "\n",
        "        # use viterbi algorithm to decode the correct sequence, given \"y\" sequence of tags \n",
        "        # from the bilstm and given the transition matrix that we have\n",
        "\n",
        "        # VITERBI ALGORITHM - output = decoded_tag_sequence of shape (batchsize, seqlen)\n",
        "        forward_scores = self.init_for_scores(Ylstm)\n",
        "        backtrack = []\n",
        "        for i in range(Ylstm.shape[1]):\n",
        "            for_scr_temp = forward_scores.view((Ylstm.shape[0], 1, Ylstm.shape[2])) \n",
        "            forward_transition_score_word = self.transmat + for_scr_temp \n",
        "\n",
        "            max_indices = torch.argmax(forward_transition_score_word, dim = 2)\n",
        "            forward_transition_score_word , _ = torch.max(forward_transition_score_word, dim = 2) # max_indices of shape (batches, C)\n",
        "            backtrack.append(max_indices)\n",
        "            unaryscore_word = Ylstm[:, i]#.view(Ylstm.shape[0], 1, Ylstm.shape[2]) # unary_score_word  of shape (batchsize, 1, num_classes)\n",
        "            forward_transition_score_word = forward_transition_score_word + unaryscore_word\n",
        "            \n",
        "            forward_scores = forward_transition_score_word * Ymask[:, i].view(Ylstm.shape[0], 1) + (1 - Ymask[:, i].view(Ylstm.shape[0], 1)) * forward_scores\n",
        "\n",
        "        end_score = forward_scores + self.transmat[self.endid] #shape will be (batches, num_classes)\n",
        "        max_index = torch.argmax(end_score, dim = 1).view(Ylstm.shape[0], 1) # shape is (Batch, 1)\n",
        "        # print(max_index.shape)\n",
        "        decoded_path = [max_index] \n",
        "\n",
        "        for index in backtrack[::-1]: #index is of shape (batches, numclasses)\n",
        "            # print(max_index)\n",
        "            max_index = torch.gather(index, 1, max_index).view(Ylstm.shape[0], 1)\n",
        "            decoded_path.append(max_index)\n",
        "        decoded_path = decoded_path[::-1][1:] #remove the start word\n",
        "        # print(decoded_path)\n",
        "        decoded_path = torch.cat(decoded_path, dim = 1)\n",
        "        return decoded_path\n",
        "\n",
        "    def crfloss(self, Ylstm, Ytrue, Ymask):\n",
        "        # loss is the negative los likelyhood = partition function - numerator\n",
        "        partition, numerator  = self.get_partition_numerator(Ylstm, Ytrue, Ymask) #numerator means in log space here\n",
        "        loss = partition - numerator\n",
        "        # print(numerator.mean(), partition.mean())\n",
        "        return loss.mean()\n",
        "    \n",
        "    def get_partition_numerator(self, Ylstm, Ytrue, Ymask):\n",
        "        # calculate the partition function\n",
        "        forward_scores = self.init_for_scores(Ylstm)\n",
        "        for i in range(Ylstm.shape[1]):\n",
        "            unaryscore_word = Ylstm[:, i].view(Ylstm.shape[0], Ylstm.shape[2], 1) # shape is now (batchsize, num_classes, 1)\n",
        "            unary_transition_score_word = unaryscore_word + self.transmat.view(1, Ylstm.shape[2], Ylstm.shape[2])\n",
        "            temp_score = torch.logsumexp(unary_transition_score_word + forward_scores.view(Ylstm.shape[0], 1, Ylstm.shape[2]), dim = 2)\n",
        "            forward_scores = temp_score * (Ymask[:, i].view(Ylstm.shape[0], 1)) + (1- Ymask[:, i].view(Ylstm.shape[0], 1)) * forward_scores\n",
        "\n",
        "        end_score_ = forward_scores + self.transmat[self.endid].view(1, Ylstm.shape[2])\n",
        "        forward_scores = torch.logsumexp(end_score_, dim = 1)\n",
        "\n",
        "        # calculate the score using Ylstm, Ytrue - shapes (batch, seqlen, numclass), (batch, seqlen)\n",
        "        # Ytrue is the tags of shape (Batch, seqlen), we concatenate start tag with this\n",
        "        Ytrue_mod = self.concat_start_tag_begin(Ytrue)\n",
        "        unary_scores = self.get_emission_score(Ylstm, Ytrue)\n",
        "        transition_scores = self.get_transition_score(Ytrue_mod)\n",
        "        total_score = ((unary_scores+transition_scores)*Ymask).sum(1) # sum alon the length dimension, finally get a (batch, 1) dimesnion vector\n",
        "        end_score = self.get_stop_score(Ytrue_mod)\n",
        "        numerator = total_score + end_score # shape is (batchsize)\n",
        "        # we have the numerator in log space\n",
        "        return forward_scores, numerator\n",
        "        \n",
        "    def init_for_scores(self, Ylstm):\n",
        "        forward_scores = torch.ones((Ylstm.shape[0], Ylstm.shape[2]))*-10000.\n",
        "        forward_scores = forward_scores.long().to(Ylstm.device)\n",
        "        forward_scores[:, self.startid] = 0.\n",
        "        return forward_scores\n",
        "\n",
        "    def get_stop_score(self, Ytrue):\n",
        "        from_ = Ytrue[:, -1].long()\n",
        "        to_ = self.endid\n",
        "        stop_scores = self.transmat[to_, from_]\n",
        "        return stop_scores\n",
        "\n",
        "    def concat_start_tag_begin(self, Ytrue):\n",
        "        starttags_batch = torch.full((Ytrue.shape[0], 1), fill_value = self.startid).to(Ytrue.device)\n",
        "        Ytrue = torch.cat((starttags_batch, Ytrue), dim = 1)\n",
        "        return Ytrue\n",
        "\n",
        "    def get_emission_score(self, Ylstm, Ytrue):\n",
        "        shape = Ytrue.shape\n",
        "        truetags = Ytrue.view(shape[0], shape[1], 1)\n",
        "        emission_scores = Ylstm.gather(dim=2, index=truetags.type(torch.int64).to(Ylstm.device)).view(shape[0], shape[1])\n",
        "        return emission_scores\n",
        "    \n",
        "    def get_transition_score(self, Ytrue):\n",
        "        from_ = Ytrue[:, :-1].long()\n",
        "        to_ = Ytrue[:, 1:].long()\n",
        "        transition_scores = self.transmat[to_, from_]\n",
        "        return transition_scores # of shape (batch, seqlen)\n",
        "\n",
        "\"\"\"# LSTM model for character **level**\n",
        "\"\"\"\n",
        "class forLSTM(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, pretr_char_embed):\n",
        "        super(forLSTM, self).__init__()\n",
        "        self.charembed = nn.Embedding.from_pretrained(pretr_char_embed, freeze = False) #size of pretrained = (totalchars,embedding size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, bidirectional = True, batch_first = True)\n",
        "\n",
        "    def forward(self, xchar, xlength_char):\n",
        "        #xchar is of shape(batchsize, seqlen_maxinbatch, maxwordlen-ie max char = 6)\n",
        "\n",
        "        shape = xchar.shape\n",
        "        xchar = xchar.view(-1, shape[2])\n",
        "        xlength_char = xlength_char.view(-1)\n",
        "        input = pack_padded_sequence(xchar, xlength_char.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        input, _ = pad_packed_sequence(input, batch_first=True)\n",
        "        embed = self.charembed(input)\n",
        "        _, (h,_) = self.lstm(embed) #h is of size (2, 128*maxno. of words in a sentence in the batch, 25)\n",
        "        h = h.view(h.shape[1], 50)\n",
        "        h = h.view(shape[0], shape[1], 50)\n",
        "        return h\n",
        "\n",
        "\"\"\"# BiLSTM Model\"\"\"\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, total_words, num_class, pretrained = False, pretrained_embed = None, char_embed_size = 0, pretr_char_embed = None):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.wordembed = nn.Embedding.from_pretrained(pretrained_embed, freeze = False)\n",
        "        self.for_charembed = forLSTM(embedding_size = char_embed_size, hidden_size = 25, pretr_char_embed = pretr_char_embed)\n",
        "        self.dropout = nn.Dropout(p = 0.5)\n",
        "        self.bilstm = LSTM(embedding_size + 50,hidden_size, bidirectional = True, batch_first = True)\n",
        "        self.linear = nn.Linear(2*hidden_size, num_class+2) # 2 because forward and backward concatenate, +2 for feeding it into the crf layer, ie for start stop tags\n",
        "        self.crfmodule = CRFmodule(num_class)\n",
        "\n",
        "    # def forward(self, x, xchar, xlengths, xlength_char):\n",
        "    #     x = pack_padded_sequence(x, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    #     x, _ = pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "    #     xlength_char = pack_padded_sequence(xlength_char, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    #     xlength_char, _ = pad_packed_sequence(xlength_char, batch_first=True, padding_value = len(\"<pad>\")) \n",
        "    #     # above this line padding value is taken as len of pad word becasue that is what we pad sentences \n",
        "    #     # with hance at a character level it should be the length\n",
        "\n",
        "    #     xchar = pack_padded_sequence(xchar, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    #     xchar, _ = pad_packed_sequence(xchar, batch_first=True)\n",
        "\n",
        "    #     word_embedding = self.wordembed(x) # x is of size(batchsize, seq_len), wordembed is of size (batchsize, seq_len, embedding_size = 100)\n",
        "    #     forwardchar= self.for_charembed(xchar, xlength_char) #forwardchar and backwardchar would be of size (batchsize, seqlen. embedding_size = 25each) \n",
        "    #     word_embedding = torch.cat((word_embedding, forwardchar), dim = 2)\n",
        "\n",
        "    #     word_embedding = self.dropout(word_embedding) #dropout\n",
        "    #     out, (h,c) = self.bilstm(word_embedding) #'out' has dimension(batchsize, seq_len, 2*hidden_size)\n",
        "\n",
        "    #     out = self.linear(out) #now 'out' has dimension(batchsize, seq_len, num_class)\n",
        "    #     out = out.view(-1, out.shape[2]) # shape (128*seqlen, 18)\n",
        "    #     out = F.log_softmax(out, dim=1) # take the softmax across the dimension num_class, 'out' has dimension(batchsize, seq_len, num_class)\n",
        "    #     return out\n",
        "        \n",
        "    def forward(self, x, xchar, xlengths, xlength_char, xmask): \n",
        "        xmask = xmask.to(x.device)\n",
        "        ylstm, xmask = self.lstmoutput(x, xchar, xlengths, xlength_char, xmask)\n",
        "        out = self.crfmodule(ylstm, xmask) #out of shape (batch, seqlen)\n",
        "        return out\n",
        "\n",
        "    def loss(self, x, xchar, xlengths, xlength_char, ytrue, xmask):\n",
        "        xmask = xmask.to(x.device)\n",
        "\n",
        "        '''calls the loss function of the crf for getting the negative log likelyhood loss'''\n",
        "        ylstm, xmask = self.lstmoutput(x, xchar, xlengths, xlength_char, xmask)\n",
        "        loss = self.crfmodule.crfloss(ylstm, ytrue, xmask)\n",
        "        return loss\n",
        "\n",
        "    def lstmoutput(self, x, xchar, xlengths, xlength_char, xmask):\n",
        "        x = pack_padded_sequence(x, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        x, _ = pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "        xlength_char = pack_padded_sequence(xlength_char, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        xlength_char, _ = pad_packed_sequence(xlength_char, batch_first=True, padding_value = len(\"<pad>\")) \n",
        "        # above this line padding value is taken as len of pad word becasue that is what we pad sentences \n",
        "        # with hance at a character level it should be the length\n",
        "\n",
        "        xchar = pack_padded_sequence(xchar, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        xchar, _ = pad_packed_sequence(xchar, batch_first=True)\n",
        "\n",
        "        xmask = pack_padded_sequence(xmask, xlengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        xmask, _ = pad_packed_sequence(xmask, batch_first=True)\n",
        "\n",
        "        word_embedding = self.wordembed(x) # x is of size(batchsize, seq_len), wordembed is of size (batchsize, seq_len, embedding_size = 100)\n",
        "        forwardchar= self.for_charembed(xchar, xlength_char) #forwardchar and backwardchar would be of size (batchsize, seqlen. embedding_size = 25each) \n",
        "        word_embedding = torch.cat((word_embedding, forwardchar), dim = 2)\n",
        "\n",
        "        word_embedding = self.dropout(word_embedding) # dropout\n",
        "        out, (h,c) = self.bilstm(word_embedding) # 'out' has dimension(batchsize, seq_len, 2*hidden_size)\n",
        "        out = self.linear(out) # now 'out' has dimension(batchsize, seq_len, num_class+2)\n",
        "\n",
        "        return out, xmask\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmbNAAid2kRA"
      },
      "source": [
        "model = BiLSTM(embedding_size = 100, hidden_size = 100, total_words = len(vocab), num_class = 18, pretrained = True, pretrained_embed = word_embeds, char_embed_size = len(char_vocab), pretr_char_embed = char_onehot).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3) \n",
        "lossfunction = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# print(model)\n",
        "def performance(y, ypred, nertags):\n",
        "    y = y.numpy()\n",
        "    ypred = ypred.numpy()\n",
        "    mask = (y != nertags['padtag']) * (y != nertags['O'])\n",
        "    y = y*mask\n",
        "    ypred = ypred*mask\n",
        "    acc = ((y==ypred)*mask).sum()/mask.sum()\n",
        "    microf1 = f1_score(y, ypred, labels = imp_classes, average='micro')\n",
        "    macrof1 = f1_score(y, ypred, labels = imp_classes, average='macro')\n",
        "    return acc, microf1, macrof1\n",
        "\n",
        "def validate(model, loader):\n",
        "    with torch.no_grad():\n",
        "        validloss = 0\n",
        "        acc = 0\n",
        "        microf1 = 0\n",
        "        macrof1 = 0\n",
        "        i = 0\n",
        "        for step, (X, Xchar, Y, xlen, xlen_char, Xmask) in enumerate(loader):\n",
        "            Y = pack_padded_sequence(Y, xlen, batch_first=True, enforce_sorted=False)\n",
        "            Y, _ = pad_packed_sequence(Y, batch_first=True)\n",
        "            ypred = model(X.long().to(device), Xchar.to(device), xlen.to(device), xlen_char.to(device), Xmask)#.permute(0, 2, 1)\n",
        "            vloss = model.loss(X.long().to(device), Xchar.to(device), xlen.to(device), xlen_char.to(device), Y, Xmask)\n",
        "            validloss+=vloss.item()\n",
        "            acc_, microf1_, macrof1_ = performance(Y.view(-1), ypred.to('cpu').view(-1), nertags)\n",
        "            acc+=acc_\n",
        "            microf1 += microf1_\n",
        "            macrof1 += macrof1_\n",
        "            i+=1\n",
        "\n",
        "    return validloss/i, acc/i, microf1/i, macrof1/i\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gLyMWbwL2-kr",
        "outputId": "174dd59d-896c-4ef6-f1d9-74f777b2ceec"
      },
      "source": [
        "trainlosslist = []\n",
        "trainacclist = [] #accuracy except pad, O\n",
        "trainmicrof1list = []\n",
        "trainmacrof1list = []\n",
        "\n",
        "\n",
        "validlosslist = []\n",
        "valacclist = []\n",
        "valmicrof1list = []\n",
        "valmacrof1list = []\n",
        "\n",
        "\n",
        "# Model is ready now we have to train using cross entropy loss\n",
        "num_epochs = 50\n",
        "# validloss = []\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    if(epoch == 35):\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "    \n",
        "    totalloss, acc, microf1, macrof1 = 0, 0, 0, 0\n",
        "    for step, (Xbatch , Xchar ,Ybatch, xbatch_len, xlength_char, xbatch_mask) in enumerate(Trainloader):\n",
        "        #make gradients 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        Ybatch = pack_padded_sequence(Ybatch, xbatch_len, batch_first=True, enforce_sorted=False)\n",
        "        Ybatch, y_lengths = pad_packed_sequence(Ybatch, batch_first=True)\n",
        "\n",
        "        #get output from model and claculate loss\n",
        "        ypred = model(Xbatch.long().to(device), Xchar.to(device), xbatch_len.to(device), xlength_char.to(device), xbatch_mask)#.permute(0, 2, 1)\n",
        "        loss = model.loss(Xbatch.long().to(device), Xchar.to(device), xbatch_len.to(device), xlength_char.to(device), Ybatch, xbatch_mask)\n",
        "        \n",
        "        acc_, microf1_, macrof1_ = performance(Ybatch.view(-1), ypred.to('cpu').view(-1), nertags)\n",
        "        acc+= acc_\n",
        "        microf1+=microf1_\n",
        "        macrof1+=macrof1_\n",
        "        # if(step%20 == 0 and step !=0):\n",
        "            # print(\"training accuracy = {}, microF1 = {}, macroF1 = {}\".format(acc/(step+1), microf1/(step+1), macrof1/(step+1)))\n",
        "        \n",
        "        totalloss += loss.item()\n",
        "\n",
        "        #backward and step\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 5) # clip gradient to 5\n",
        "        optimizer.step()\n",
        "    \n",
        "    trainlosslist.append(totalloss/(step+1))\n",
        "    trainacclist.append(acc/(step+1))\n",
        "    trainmicrof1list.append(microf1/(step+1))\n",
        "    trainmacrof1list.append(macrof1/(step+1))\n",
        "\n",
        "    # model validation loss and scheduler step for learning rate change if required\n",
        "    val_loss, val_acc, val_microf1, val_macrof1  = validate(model, Devloader)\n",
        "    validlosslist.append(val_loss)\n",
        "    valacclist.append(val_acc)\n",
        "    valmicrof1list.append(val_microf1)\n",
        "    valmacrof1list.append(val_macrof1)\n",
        "    \n",
        "    # scheduler.step(val_loss)\n",
        "    print('\\nepoch = {}, training_loss = {}, validation_loss ={}, training_acc = {}, validation_acc ={}'.format(epoch, trainlosslist[-1], validlosslist[-1], trainacclist[-1], valacclist[-1]))        \n",
        "        \n",
        "\n",
        "model.eval()\n",
        "\n",
        "import os\n",
        "if not os.path.exists(rootpath):\n",
        "    os.mkdir(rootpath)\n",
        "\n",
        "if not os.path.exists(rootpath+Expname):\n",
        "    os.mkdir(rootpath+Expname)\n",
        "\n",
        "\n",
        "def SavePlots(y1, y2, metric, rootpath, Expname):\n",
        "    try:\n",
        "        plt.clf()\n",
        "    except Exception as e:\n",
        "        pass\n",
        "    \"\"\"y2 should be validation\"\"\"\n",
        "    epochs=np.arange(1,len(y1)+1,1)\n",
        "    plt.title(Expname + \" \" + metric + \" plot\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric)\n",
        "    plt.plot(epochs,y1,label='Training %s'%metric, linewidth = 2)\n",
        "    plt.plot(epochs,y2,label='Validation %s'%metric, linewidth = 2)\n",
        "    if(metric == \"Loss\"):\n",
        "        ep=np.argmin(y2)\n",
        "    elif(metric != \"Loss\"):\n",
        "        ep =np.argmax(y2)\n",
        "    plt.plot(ep+1,y2[ep],'r*',label='bestvalue@(%.i,%.2f)'%(ep+1,y2[ep]))\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.savefig(rootpath+Expname+\"/{}\".format(metric), dpi=300)\n",
        "\n",
        "SavePlots(trainlosslist, validlosslist, \"Loss\", rootpath, Expname)\n",
        "SavePlots(trainacclist, valacclist, \"Accuracy\", rootpath, Expname)\n",
        "SavePlots(trainmicrof1list, valmicrof1list, \"Micro F1\", rootpath, Expname)\n",
        "SavePlots(trainmacrof1list, valmacrof1list, \"Macro F1\", rootpath, Expname)\n",
        "\n",
        "#make id2tag\n",
        "id2tag = {}\n",
        "for tag in nertags.keys():\n",
        "    if(tag == 'padtag'):\n",
        "        id2tag[nertags[tag]] = 'O' # because we dont want the model to predict 'padtag' tags\n",
        "    else:\n",
        "        id2tag[nertags[tag]] = tag\n",
        "\n",
        "\n",
        "def final_metrics(model, loader):\n",
        "    y_predicted = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for step, (X, Xchar, Y, xlen, xlen_char, xmask) in enumerate(loader):\n",
        "            Y = pack_padded_sequence(Y, xlen, batch_first=True, enforce_sorted=False)\n",
        "            Y, _ = pad_packed_sequence(Y, batch_first=True)\n",
        "            ypred = model(X.long().to(device), Xchar.to(device), xlen.to(device), xlen_char.to(device), xmask)#.permute(0, 2, 1)\n",
        "            # ypred = torch.argmax(ypred.to('cpu'), dim = 1)\n",
        "            ypred = ypred.view(Y.shape[0], -1)\n",
        "            # print(ypred.shape)\n",
        "\n",
        "            y_predicted.append(ypred)\n",
        "            y_true.append(Y)\n",
        "\n",
        "    y_predicted_list = []\n",
        "    y_true_list = []\n",
        "    for i in range(len(y_predicted)):\n",
        "        for j in range(y_predicted[i].shape[0]):\n",
        "            sent_pred = []\n",
        "            sent_true = []\n",
        "            for x in range(y_predicted[i].shape[1]):\n",
        "                sent_pred.append(id2tag[int(y_predicted[i][j, x])])\n",
        "                sent_true.append(id2tag[int(y_true[i][j, x])])\n",
        "            y_predicted_list.append(sent_pred)\n",
        "            y_true_list.append(sent_true)\n",
        "    # print(y_predicted_list[0:5])\n",
        "    # print(y_true_list[0:5])\n",
        "    return seq_f1_score(y_true_list, y_predicted_list), seq_accuracy_score(y_true_list, y_predicted_list), seq_classification_report(y_true_list, y_predicted_list, digits = 3)\n",
        "    #CONVERTING y_predicted and y_true lists into tag list\n",
        "    # return y_predicted, y_true\n",
        "\n",
        "\n",
        "# calculate the final metrics usign seq eval\n",
        "# TRAINING DATA\n",
        "loader_train = DataLoader(traindataset, batch_size= 1, shuffle=False)\n",
        "train_f1_conll, train_acc_conll, train_classif_report = final_metrics(model, loader_train)\n",
        "\n",
        "# VALIDATION DATA\n",
        "loader_valid = DataLoader(devdataset, batch_size= 1, shuffle=False)\n",
        "valid_f1_conll, valid_acc_conll, valid_classif_report = final_metrics(model, loader_valid)\n",
        "\n",
        "print(\"PERFORMANCE ON Train DATA\")\n",
        "print('MicroF1 = {} '.format(train_f1_conll))\n",
        "print('Accuracy = {}'.format(train_acc_conll))\n",
        "print('------------Classification Report-------------')\n",
        "print(train_classif_report)\n",
        "\n",
        "print(\"PERFORMANCE ON Validation DATA\")\n",
        "print('MicroF1 = {} '.format(valid_f1_conll))\n",
        "print('Accuracy = {}'.format(valid_acc_conll))\n",
        "print('------------Classification Report-------------')\n",
        "print(valid_classif_report)\n",
        "\n",
        "\n",
        "\n",
        "#Test DATASET\n",
        "testdatapath = \"/content/drive/MyDrive/Q2_DL/test.txt\"\n",
        "char_vocab = get_charvocab(vocab)\n",
        "Xtest, Ytest, x_testlengths, _, _ = load_data(testdatapath, buildvocab_tags=False, vocab = vocab, nertags = nertags)\n",
        "bin_mask_test =  get_mask(Xtest, x_testlengths)\n",
        "\n",
        "Xtest_temp, Ytest_temp, x_testlengths_temp, testvocab, testnertags = load_data(testdatapath, buildvocab_tags=True)\n",
        "wordid2word_charlevel_vocab_test, wordid2wordlen_test = make_id2word_charvocab(testvocab, char_vocab) # of the form {word:[1,2,3,4]}, {wordnum:wordlen}\n",
        "#make char level train data for the char embeddings \n",
        "Xtest_char, xtestlength_char = load_char_level(Xtest_temp, wordid2word_charlevel_vocab_test, wordid2wordlen_test)\n",
        "#finally make the dataloader for train\n",
        "testdataset = TensorDataset(Xtest, Xtest_char, Ytest, x_testlengths, xtestlength_char, bin_mask_test)\n",
        "loader_test = DataLoader(testdataset, batch_size= 1, shuffle=False)\n",
        "test_f1_conll, test_acc_conll, test_classif_report = final_metrics(model, loader_test)\n",
        "\n",
        "\n",
        "\n",
        "print(\"PERFORMANCE ON Test DATA\")\n",
        "print('MicroF1 = {}'.format(test_f1_conll))\n",
        "print('Accuracy = {}'.format(test_acc_conll))\n",
        "print('------------Classification Report-------------')\n",
        "print(test_classif_report)\n",
        "\n",
        "\n",
        "\"\"\"SAVING DATA\"\"\"\n",
        "\n",
        "# save performance metrics dictionaries\n",
        "# save train loss, acc, micro, macro\n",
        "# save val loss, acc, micro, macro\n",
        "# save model\n",
        "import pickle\n",
        "#train\n",
        "pickle.dump(train_classif_report, open(rootpath+Expname+\"/train_classif_report.dict.pickle\", \"wb\" ))\n",
        "np.save(rootpath+Expname+\"/train_losslist.npy\", np.asarray(trainlosslist))\n",
        "np.save(rootpath+Expname+\"/train_acclist.npy\", np.asarray(trainacclist))\n",
        "np.save(rootpath+Expname+\"/train_microf1list.npy\", np.asarray(trainmicrof1list))\n",
        "np.save(rootpath+Expname+\"/train_macrof1list.npy\", np.asarray(trainmacrof1list))\n",
        "\n",
        "#valid\n",
        "pickle.dump(valid_classif_report, open(rootpath+Expname+\"/valid_classif_report.dict.pickle\", \"wb\" ))\n",
        "np.save(rootpath+Expname+\"/val_losslist.npy\", np.asarray(validlosslist))\n",
        "np.save(rootpath+Expname+\"/val_acclist.npy\", np.asarray(valacclist))\n",
        "np.save(rootpath+Expname+\"/val_microf1list.npy\", np.asarray(valmicrof1list))\n",
        "np.save(rootpath+Expname+\"/val_macrof1list.npy\", np.asarray(valmacrof1list))\n",
        "\n",
        "#test\n",
        "pickle.dump(test_classif_report, open(rootpath+Expname+\"/test_classif_report.dict.pickle\", \"wb\" ))\n",
        "\n",
        "\n",
        "#Save Model\n",
        "torch.save(model, rootpath+Expname+\"/{}_model.pth\".format(Expname))    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch = 0, training_loss = 7.362317047577953, validation_loss =3.6335435159427605, training_acc = 0.48517809200267165, validation_acc =0.6809782343804102\n",
            "\n",
            "epoch = 1, training_loss = 2.614925865045528, validation_loss =2.169143346167102, training_acc = 0.7133947967033853, validation_acc =0.7155972811148226\n",
            "\n",
            "epoch = 2, training_loss = 1.4190568159945642, validation_loss =1.183570728781297, training_acc = 0.7487177854336481, validation_acc =0.7456446032291303\n",
            "\n",
            "epoch = 3, training_loss = 0.48799098349621206, validation_loss =0.3800862557494763, training_acc = 0.7703092493768585, validation_acc =0.7611417033236015\n",
            "\n",
            "epoch = 4, training_loss = -0.3283362939595357, validation_loss =-0.3540545139730591, training_acc = 0.7826948521906116, validation_acc =0.7710803726428569\n",
            "\n",
            "epoch = 5, training_loss = -1.0954719557590091, validation_loss =-1.0801305804670471, training_acc = 0.7950343206355539, validation_acc =0.7717513903699194\n",
            "\n",
            "epoch = 6, training_loss = -1.8215102616863972, validation_loss =-1.7547937613172628, training_acc = 0.8018649150104351, validation_acc =0.7758361854576294\n",
            "\n",
            "epoch = 7, training_loss = -2.5137762281902876, validation_loss =-2.417931280185267, training_acc = 0.8089875738449593, validation_acc =0.7768580335795489\n",
            "\n",
            "epoch = 8, training_loss = -3.1850228661933717, validation_loss =-3.0606267329343817, training_acc = 0.8160634643050295, validation_acc =0.7819361479319866\n",
            "\n",
            "epoch = 9, training_loss = -3.8539793737155876, validation_loss =-3.636650830200038, training_acc = 0.8218175460767556, validation_acc =0.782779677430252\n",
            "\n",
            "epoch = 10, training_loss = -4.5024146583072095, validation_loss =-4.264886856079102, training_acc = 0.8273483041370288, validation_acc =0.7827408771538338\n",
            "\n",
            "epoch = 11, training_loss = -5.1465568935748225, validation_loss =-4.867031559501727, training_acc = 0.8313862537535669, validation_acc =0.7826311652629796\n",
            "\n",
            "epoch = 12, training_loss = -5.785239032863342, validation_loss =-5.508759916443186, training_acc = 0.836070441151265, validation_acc =0.7869427046932644\n",
            "\n",
            "epoch = 13, training_loss = -6.404945796297998, validation_loss =-6.0458820333185885, training_acc = 0.8412249339120814, validation_acc =0.7798647547200628\n",
            "\n",
            "epoch = 14, training_loss = -7.028555951986935, validation_loss =-6.631506457771223, training_acc = 0.844876604550899, validation_acc =0.7778814107907934\n",
            "\n",
            "epoch = 15, training_loss = -7.6524908223103, validation_loss =-7.244259008427256, training_acc = 0.8486485137853993, validation_acc =0.785020926952804\n",
            "\n",
            "epoch = 16, training_loss = -8.265790642741619, validation_loss =-7.805507502605006, training_acc = 0.8527063855972127, validation_acc =0.7805631237005967\n",
            "\n",
            "epoch = 17, training_loss = -8.883776926912393, validation_loss =-8.420349386549487, training_acc = 0.8543721448064477, validation_acc =0.7888511136051967\n",
            "\n",
            "epoch = 18, training_loss = -9.492859656868111, validation_loss =-9.003957964710354, training_acc = 0.8580453296512202, validation_acc =0.7952878425674383\n",
            "\n",
            "epoch = 19, training_loss = -10.104122932014596, validation_loss =-9.546667492266783, training_acc = 0.8632088290070123, validation_acc =0.7921677846939953\n",
            "\n",
            "epoch = 20, training_loss = -10.713783801626095, validation_loss =-10.147325771371113, training_acc = 0.8654743347019875, validation_acc =0.7900990774653163\n",
            "\n",
            "epoch = 21, training_loss = -11.309252529209832, validation_loss =-10.720793576584649, training_acc = 0.867946648883687, validation_acc =0.7991749663760059\n",
            "\n",
            "epoch = 22, training_loss = -11.922211515944438, validation_loss =-11.302214445527067, training_acc = 0.871299564190857, validation_acc =0.7975761775231097\n",
            "\n",
            "epoch = 23, training_loss = -12.52292725474564, validation_loss =-11.884645570184766, training_acc = 0.8718890483067238, validation_acc =0.7910578669704424\n",
            "\n",
            "epoch = 24, training_loss = -13.130436894000601, validation_loss =-12.451026945998988, training_acc = 0.8769961001326514, validation_acc =0.7960920437732146\n",
            "\n",
            "epoch = 25, training_loss = -13.725221548703118, validation_loss =-13.00723257753038, training_acc = 0.8778916365086068, validation_acc =0.7883186612212659\n",
            "\n",
            "epoch = 26, training_loss = -14.327428424481264, validation_loss =-13.581037187084709, training_acc = 0.8811692197196703, validation_acc =0.7933370528709324\n",
            "\n",
            "epoch = 27, training_loss = -14.914196882870598, validation_loss =-14.149958846495323, training_acc = 0.8838320871545252, validation_acc =0.7983396024342659\n",
            "\n",
            "epoch = 28, training_loss = -15.52484012551324, validation_loss =-14.704611797922665, training_acc = 0.8868455761927541, validation_acc =0.7898433739750665\n",
            "\n",
            "epoch = 29, training_loss = -16.1094394926353, validation_loss =-15.302342365697488, training_acc = 0.8875847063265045, validation_acc =0.8028347955624584\n",
            "\n",
            "epoch = 30, training_loss = -16.71067684868357, validation_loss =-15.841188460281215, training_acc = 0.8878249220428, validation_acc =0.8012186532520879\n",
            "\n",
            "epoch = 31, training_loss = -17.31402661956053, validation_loss =-16.418099029776975, training_acc = 0.889714260924956, validation_acc =0.7920605555962812\n",
            "\n",
            "epoch = 32, training_loss = -17.9170505812078, validation_loss =-16.994266234722335, training_acc = 0.8955547677173649, validation_acc =0.7925191818025471\n",
            "\n",
            "epoch = 33, training_loss = -18.504263088055904, validation_loss =-17.553048418969222, training_acc = 0.8952923402417616, validation_acc =0.7983553271851785\n",
            "\n",
            "epoch = 34, training_loss = -19.09698484361786, validation_loss =-18.104722937357796, training_acc = 0.898237453445677, validation_acc =0.785952034377703\n",
            "\n",
            "epoch = 35, training_loss = -19.461242741325876, validation_loss =-18.195144161735612, training_acc = 0.9042719612274484, validation_acc =0.7962157647093926\n",
            "\n",
            "epoch = 36, training_loss = -19.537724924251386, validation_loss =-18.264346879782135, training_acc = 0.9058713110423158, validation_acc =0.7973232767559669\n",
            "\n",
            "epoch = 37, training_loss = -19.600597047314203, validation_loss =-18.300686885401145, training_acc = 0.9076401154465705, validation_acc =0.7970974731661009\n",
            "\n",
            "epoch = 38, training_loss = -19.668265870346644, validation_loss =-18.370049112850857, training_acc = 0.9091601368741469, validation_acc =0.798868842167066\n",
            "\n",
            "epoch = 39, training_loss = -19.736172869443074, validation_loss =-18.44400183687505, training_acc = 0.9098558158040515, validation_acc =0.7980919971631455\n",
            "\n",
            "epoch = 40, training_loss = -19.797602178304874, validation_loss =-18.470864128820676, training_acc = 0.9086539344607661, validation_acc =0.7930285389719156\n",
            "\n",
            "epoch = 41, training_loss = -19.854752806044118, validation_loss =-18.50488481816557, training_acc = 0.9096337807104, validation_acc =0.79527734249096\n",
            "\n",
            "epoch = 42, training_loss = -19.920863993798744, validation_loss =-18.589036862874767, training_acc = 0.9097433861806808, validation_acc =0.8002331581184593\n",
            "\n",
            "epoch = 43, training_loss = -19.977696166415395, validation_loss =-18.65160883087473, training_acc = 0.9104762533507343, validation_acc =0.7965386872599104\n",
            "\n",
            "epoch = 44, training_loss = -20.038714530132072, validation_loss =-18.70097909514437, training_acc = 0.9117352832071379, validation_acc =0.7981362430418569\n",
            "\n",
            "epoch = 45, training_loss = -20.09848808996456, validation_loss =-18.74814127892563, training_acc = 0.9109450717132141, validation_acc =0.7961652021719337\n",
            "\n",
            "epoch = 46, training_loss = -20.170836871432275, validation_loss =-18.83366834502859, training_acc = 0.9105581675502218, validation_acc =0.7982743163327888\n",
            "\n",
            "epoch = 47, training_loss = -20.224573161593828, validation_loss =-18.873856593653098, training_acc = 0.911226143950678, validation_acc =0.7959109099236482\n",
            "\n",
            "epoch = 48, training_loss = -20.28369326116293, validation_loss =-18.92670480000604, training_acc = 0.9111757032317872, validation_acc =0.7972013299301153\n",
            "\n",
            "epoch = 49, training_loss = -20.346959654817876, validation_loss =-18.98616069125146, training_acc = 0.9119879925733199, validation_acc =0.7969470498494514\n",
            "PERFORMANCE ON Train DATA\n",
            "MicroF1 = 0.9408832687736344 \n",
            "Accuracy = 0.9901621330619439\n",
            "------------Classification Report-------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         art      0.900     0.818     0.857       296\n",
            "         eve      0.858     0.832     0.845       226\n",
            "         geo      0.933     0.968     0.950     29240\n",
            "         gpe      0.977     0.967     0.972     12058\n",
            "         nat      0.800     0.902     0.848       133\n",
            "         org      0.919     0.869     0.893     15803\n",
            "         per      0.934     0.937     0.936     13121\n",
            "         tim      0.944     0.964     0.954     15767\n",
            "\n",
            "   micro avg      0.938     0.943     0.941     86644\n",
            "   macro avg      0.908     0.907     0.907     86644\n",
            "weighted avg      0.938     0.943     0.941     86644\n",
            "\n",
            "PERFORMANCE ON Validation DATA\n",
            "MicroF1 = 0.8200622621930128 \n",
            "Accuracy = 0.964174955736184\n",
            "------------Classification Report-------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         art      0.346     0.171     0.229       105\n",
            "         eve      0.450     0.346     0.391        78\n",
            "         geo      0.834     0.868     0.851      9724\n",
            "         gpe      0.931     0.923     0.927      4210\n",
            "         nat      0.480     0.480     0.480        50\n",
            "         org      0.714     0.648     0.679      5187\n",
            "         per      0.784     0.754     0.769      4457\n",
            "         tim      0.865     0.873     0.869      5254\n",
            "\n",
            "   micro avg      0.824     0.816     0.820     29065\n",
            "   macro avg      0.676     0.633     0.649     29065\n",
            "weighted avg      0.821     0.816     0.818     29065\n",
            "\n",
            "PERFORMANCE ON Test DATA\n",
            "MicroF1 = 0.8200697390796503\n",
            "Accuracy = 0.9640398339388417\n",
            "------------Classification Report-------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         art      0.233     0.098     0.138       102\n",
            "         eve      0.397     0.356     0.376        87\n",
            "         geo      0.840     0.871     0.855      9912\n",
            "         gpe      0.924     0.924     0.924      4168\n",
            "         nat      0.464     0.473     0.468        55\n",
            "         org      0.705     0.644     0.673      5205\n",
            "         per      0.784     0.767     0.775      4406\n",
            "         tim      0.863     0.871     0.867      5275\n",
            "\n",
            "   micro avg      0.823     0.817     0.820     29210\n",
            "   macro avg      0.651     0.625     0.635     29210\n",
            "weighted avg      0.819     0.817     0.818     29210\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_79k6ok9O_q"
      },
      "source": [
        "class LayerNorm_LSTMCell(nn.Module):\n",
        "  def __init__(self, embedding_size, hidden_size, epsilon=1e-5):\n",
        "    super(LayerNorm_LSTMCell, self).__init__()\n",
        "    self.embedding_size=embedding_size\n",
        "    self.hidden_size=hidden_size\n",
        "    self.epsilon=1e-5\n",
        "\n",
        "    self.inp2hidden = nn.Linear(embedding_size, 4*hidden_size) #since there are 4 linear transformations : i,f,g,o\n",
        "    self.hidden2hidden = nn.Linear(hidden_size, 4*hidden_size) #this takes input h_t_minus_1\n",
        "\n",
        "    # define layernorm layers : we are normalising input to input,forget,output,cell gate in \"cummulative fashion\"\n",
        "    self.layernorm_inp2hidden = nn.LayerNorm(4*hidden_size)\n",
        "    self.layernorm_hidden2hidden = nn.LayerNorm(4*hidden_size)\n",
        "    self.layernorm_over_c = nn.LayerNorm(hidden_size)\n",
        "   \n",
        "  def forward(self, inp, h_c_prev=None): # x (batch,embedding_size), h_c (h and c at t-1) : [(batch,hidden_size)]*2\n",
        "    h_prev, c_prev = h_c_prev if h_c_prev!=None else (torch.zeros((inp.shape[0],self.hidden_size)).to(device) , torch.zeros((inp.shape[0],self.hidden_size)).to(device))\n",
        "    # first do the linear transformations with layernorms\n",
        "    linear_transform_inp = self.layernorm_inp2hidden(self.inp2hidden(inp)) + self.layernorm_hidden2hidden(self.hidden2hidden(h_prev))\n",
        "\n",
        "    #find the gates\n",
        "    split=self.hidden_size\n",
        "    i=torch.sigmoid(linear_transform_inp[:,0:split])\n",
        "    f=torch.sigmoid(linear_transform_inp[:,split:2*split])\n",
        "    o=torch.sigmoid(linear_transform_inp[:,2*split:3*split])\n",
        "    g=torch.tanh(linear_transform_inp[:,3*split:])\n",
        "    c=f*c_prev+i*g\n",
        "\n",
        "    #apply final layer norm\n",
        "    c=self.layernorm_over_c(c)\n",
        "    h=o*torch.tanh(c)\n",
        "    return (h,c)\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, embedding_size, hidden_size, bidirectional=True, batch_first=True):\n",
        "    super(LSTM, self).__init__()\n",
        "    # bidirectional=True, batch_first=True : always (assumed)\n",
        "    self.embedding_size=embedding_size\n",
        "    self.hidden_size=hidden_size\n",
        "    self.lstm_cell_fwd=LayerNorm_LSTMCell(embedding_size, hidden_size)\n",
        "    self.lstm_cell_bkwd=LayerNorm_LSTMCell(embedding_size, hidden_size)\n",
        "\n",
        "  def forward(self, inp): #inp (batch,seq_len,embedding_size)\n",
        "    # iterate over seq_len in forward and backward direction (using their respective lstm cells)\n",
        "\n",
        "    ht_fwd, ht_bkwd = [],[]\n",
        "    ct_fwd, ct_bkwd = [],[]\n",
        "    \n",
        "    for t in range(inp.shape[1]):\n",
        "      #forward lstm\n",
        "      h_c_prev=(ht_fwd[t-1],ct_fwd[t-1]) if t!=0 else None\n",
        "      ht,ct=self.lstm_cell_fwd(inp[:,t,:],h_c_prev)\n",
        "      ht_fwd.append(ht)\n",
        "      ct_fwd.append(ct)\n",
        "\n",
        "      #backward lstm\n",
        "      t_ = inp.shape[1]-t-1\n",
        "      h_c_prev_ = (ht_bkwd[0],ct_bkwd[0]) if t_!=inp.shape[1]-1 else None\n",
        "      ht_,ct_=self.lstm_cell_bkwd(inp[:,t_,:],h_c_prev_)\n",
        "      ht_bkwd.insert(0,ht_)\n",
        "      ct_bkwd.insert(0,ct_)\n",
        "    \n",
        "    #now stack ht and give it as out\n",
        "    #also take last ht,ct of fwd lstm and first ht,ct of bkwd lstm\n",
        "    #and give it as hn,cn after stacking\n",
        "\n",
        "    ht_fwd=torch.stack(ht_fwd) #seq_len,batch,hidden_dim\n",
        "    ct_fwd=torch.stack(ct_fwd) #seq_len,batch,hidden_dim\n",
        "    ht_bkwd=torch.stack(ht_bkwd) #seq_len,batch,hidden_dim\n",
        "    ct_bkwd=torch.stack(ct_bkwd) #seq_len,batch,hidden_dim\n",
        "    out=torch.cat((ht_fwd,ht_bkwd),dim=2).permute(1,0,2) #batch,seq_len,2*hidden_dim\n",
        "    \n",
        "    hn=torch.stack((ht_fwd[-1],ht_bkwd[0])) #batch,hidden_dim\n",
        "    cn=torch.stack((ct_fwd[-1],ct_bkwd[0])) #batch,hidden_dim\n",
        "    return out,(hn,cn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEMexKjZs-cX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}